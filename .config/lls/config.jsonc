{
  // 공통 기본값
  "defaults": {
    "host": "0.0.0.0",
    "port": 8080,

    // llama-server: --ctx-size
    "ctx": 40960,

    // llama-server: --threads
    "threads": 6,

    // llama-server: --batch-size / --ubatch-size
    "batch": 2048,
    "ubatch": 256,

    // mac metal: -1 / nvidia cuda: 적당히
    // llama-server: --n-gpu-layers
    "gpu_layers": -1
    // "flag_flash_attn": 1  

    // 필요하면 여기에 raw 옵션을 추가할 수도 있음:
    // "extra_args": ["--no-mmap"]
    //
    // 또는 flag_* 형태로 boolean/값 플래그를 쓸 수도 있음:
    // "flag_no_mmap": true          -> --no-mmap
    // "flag_flash_attn": 1          -> --flash-attn 1
  },

  // 이 프로필들은 "모델 매칭" 결과로 합쳐짐(override)
  "profiles": {
    "mac": {
      "gpu_layers": -1,
      "threads": 4
    },
    "mac2": {
      "ctx": 20480,
      "gpu_layers": -1,
      "threads": 4
    },
    "wsl": {
      "gpu_layers": -1,
      "threads": 8
    },
    "ccy": {
      "gpu_layers": -1,
      "threads": 6
    },
    "halo": {
      "gpu_layers": 99,
      "threads": 16
    },

    // 예: 무거운 모델은 ctx/batch 낮춰 안정화
    "safe": {
      "gpu_layers": -1,
      "ctx": 4096,
      "batch": 512,
      "ubatch": 128
    },
    "fast": {
	  "gpu_layers": 99
    }
  },

  // 모델 파일명(부분매칭/정규식 느낌) -> 적용할 프로필/직접 override
  "model_rules": [
	{
	  "name": "gemma-3-4b-ud-q4",
      "match": "gemma.*4b",
      "override": { "ctx": 40960,
					"batch": 2048,
					"ubatch": 256,
					"threads": 8,
					"extra_args":
						[
							// "--no-mmap",
							// "--flash-attn",
							// "1",
							"--chat-template-kwargs",
							"{\"enable_thinking\": false}"
						]
      }
    },
	{
	  "name": "qwen3-vl-4b",
	  "match": "qwen3.*vl.*4b.*\\.gguf$",
      "override": { "ctx": 40960,
					"batch": 2048,
					"ubatch": 256,
					"threads": 8,
					"extra_args":
						[
							// "--no-mmap",
							// "--flash-attn",
							// "1",
							"--chat-template-kwargs",
							"{\"enable_thinking\": false}"
						]
	 
	  }
	},
	{
	  "name": "glm-4.6-flash",
      "match": "glm.*flash",
      "override": {
        "extra_args":
            [
                "--no-mmap",
                "--chat-template-kwargs",
                "{\"enable_thinking\": false}"
            ]
      }
    },
	{
	  "name": "qwen3-vl-8b",
	  "match": "qwen3.*vl.*8b.*\\.gguf$",
      "override": { "ctx": 20480,
					"batch": 2048,
					"ubatch": 256,
					"threads": 8,
					"extra_args":
						[
							// "--no-mmap",
							// "--flash-attn",
							// "1",
							"--chat-template-kwargs",
							"{\"enable_thinking\": false}"
						]
	 
	  }

	  // "params": {
		// // 텍스트 모델 공통 옵션은 기본값에서 가져오고,
		// // VL 전용만 여기서 덧붙임
		// "mmproj": "/home/utylee/temp/llm_models/mmproj/qwen3-vl-mmproj.gguf",
		// "chat_template": "qwen" // 필요 시
	  // }
	},
    // {
    //   "match": "phi",
    //   "use_profiles": ["safe"]
    //   // "override": { "ctx": 4096 }
    // },
    {
      "match": "lfm",
      "use_profiles": ["fast"]
    }
    // {
    //   "match": "gpt-oss-20b",
    //   "use_profiles": ["safe"],
    //   "override": { "batch": 512, "ubatch": 128 }
    // },
    // {
    //   "match": "qwen",
    //   "use_profiles": ["defaults"]
    // },

  ]

  // (선택) 특정 모델 파일명 "정확히 일치" override
  // "overrides": {
  //   "Qwen2.5-Coder-14B-Q4_K_M.gguf": { "ctx": 8192, "batch": 1024, "ubatch": 256 }
  // }
}

